# Основные алгоритмы обучения с учителем
Типы алгоритмов:
- классификация 
- регрессия
  
Цель классификации состоит в том, чтобы спрогнозировать метку класса (class label), которая представляет собой выбор из заранее определенного списка возможных вариантов.

Цель регрессии состоит в том, чтобы спрогнозировать непрерывное число или число с плавающей точкой (floating-point number)

Виды классификации:
- бинарная
  - положительный класс (определяется объектом исслудования)
  - отрицательный класс 
- мультиклассовая

Структура данных для классификации
- классы
  - обобщенные группы в данных(сорты ростений, виды животных)
- метки
  - названия групп 
- пример(sample)/точкой данных(data point) - строка/объект в данных 
- признак(features) - столбец/свойство описывает/характеризует пример
  - высокоразмерность - большое количество признаков
  - низкоразмерность - небольшое количестов признаов
  - конструирование признаков(feature engineering)- включение производных признаков

Характеристика модели:
- переобучение(overfitting)
  - точно прогназирует на тестовых данных, делает плохой прогноз на новых данных
- недообучение(underfitting)
  - плохой прогноз на тестовых данных
- максимальная способность обобщать (generalize)
  - делает точный прогноз на новых данных

![[Vvedenie_v_mashinnoe_obuchenie_s_pomoschyu_Python_.png]]

> сложность модели тесно связана с изменчивостью входных данных, содержащихся в вашем обучающем наборе: чем больше разнообразие точек данных в вашем наборе, тем более сложную модель можно использовать, не беспокоясь о переобучении. Обычно больший объем данных дает большее разнообразие, таким образом, большие наборы данных позволяют строить более сложные модели. Однако простое дублирование одних и тех же точек данных или сбор очень похожих данных здесь не поможет.

## Метод К ближайших соседей

простейший вариант алгоритма к ближайших соседей рассматривает лишь одного ближайшего соседа-точку обучающего набора, ближе всего расположенную к точке, для которой мы хотим получить прогноз. 
В простейшем варианте алгоритм k ближайших соседей рассматривает лишь одного ближайшего соседа – точку обучающего набора, ближе всего расположенную к точке, для которой мы хотим получить прогноз. Прогнозом является ответ, уже известный для данной точки обучающего набора.

можем рассмотреть произвольное количество (k) соседей. Отсюда и происходит название алгоритма k ближайших соседей. Когда мы рассматриваем более одного соседа, для присвоения метки используется голосование (voting).

Это означает, что для каждой точки тестового набора мы подсчитываем количество соседей, относящихся к классу 0, и количество соседей, относящихся к классу 1. Затем мы присваиваем точке тестового набора наиболее часто встречающийся класс: другими словами, мы выбираем класс, набравший большинство среди k ближайших соседей.

В случае мультиклассовой классификации мы подсчитываем
количество соседей, принадлежащих к каждому классу, и снова прогнозируем наиболее часто встречающийся класс.

При использовании алгоритма ближайших соседей важно выполнить предварительную обработку данных (смотрите главу 3). Данный метод не так хорошо работает, когда речь идет о наборах данных с большим количеством признаков (сотни и более), и особенно плохо работает в ситуации, когда подавляющее число признаков в большей части наблюдений имеют нулевые значения (так называемые разреженные наборы данных или sparse datasets).

### Регрессия К ближайших соседей


## Линейные модели

Линейные модели дают прогноз, используя линейную функцию (linear function) входных признаков

Когда используется несколько признаков,
регрессионное уравнение содержит параметры наклона для каждого признака. Как вариант, прогнозируемый ответ можно представить в виде взвешенной суммы входных признаков, где веса (которые могут быть отрицательными) задаются элементами w.


Линейные модели для регрессии можно охарактеризовать как регрессионные модели, в которых прогнозом является прямая линия для одного признака, плоскость, когда используем два признака, или гиперплоскость для большего количества измерений (то есть, когда используем много признаков).


Для наборов данных с большим количеством признаков линейные модели могут быть очень полезны. В частности, если у вас количество признаков превышает количество точек данных для обучения, любую целевую переменную у можно прекрасно смоделировать (на обучающей выборке) в виде линейной функциию  

Существует различные виды линейных моделей для регрессии. Различие между этими моделями заключается в способе оценивания параметров модели w и b по обучающим данным и контроле сложности модели.  

### Линейная регрессия( обычный метод наименьших квадратов)
**Оrdinary least squares, OLS**  

Линейная регрессия находит параметры w и b, которые минимизируют среднеквадратическую ошибку (mean squared error) между спрогнозированными и фактическими ответами у в обучающем наборе.  

>  Среднеквадратичная ошибка равна сумме квадратов разностей между спрогнозированными и фактическими значениями.  


Линейная регрессия проста, что является преимуществом, но в то же время у нее нет инструментов, позволяющих контролировать сложность модели.    

Однако для высокоразмерных наборов данных (наборов данных с большим количеством признаков) линейные модели становятся более сложными и существует более высокая вероятность переобучения.  

Это несоответствие между качеством модели на обучающем наборе и качеством модели на тестовом наборе является явным признаком переобучения  

### Гребневая регрессия

Гребневая регрессия также является линейной моделью регрессии, поэтому ее формула аналогична той, что используется в обычном методе наименьших квадратов.  

В гребневой регрессии коэффициенты (w) выбираются не только с точки зрения того, насколько хорошо они позволяют предсказывать на обучающих данных, они еще подгоняются в соответствии с дополнительным ограничением.  


Нам нужно, чтобы величина коэффициентов была как можно меньше. Другими словами, все элементы w должны быть близки к нулю. Это означает, что каждый признак должен иметь как можно меньшее влияние на результат (то есть каждый признак должен иметь небольшой регрессионный коэффициент) и в то же время он должен по-прежнему обладать хорошей прогнозной силой.  

Это ограничение является примером регуляризации (regularization).   

Регуляризация означает явное ограничение модели для предотвращения переобучения. Регуляризация, использующаяся в гребневой регрессии, известна как L2 регуляризация  


### Наивные байесовские классификаторы

В scikit-learn реализованы три вида наивных байесовских классификаторов: 
- GaussianNB. можно применить к любым непрерывным данным
- BernoulliNB. принимает бинарные данные
- MultinomialNB. принимает счетные или дискретные данные

BernoulliNB и MultinomialNB в основном используются для классификации текстовых данных.  


> MultinomialNB и BernoulliNB имеют один параметр alpha, который контролирует сложность модели. Параметр alpha работает следующим образом: алгоритм добавляет к данным зависящее от alpha определенное количество искусственных наблюдений с положительными значениями для всех признаков. Это приводит к «сглаживанию» статистик. Большее значение alpha означает более высокую степень сглаживания, что приводит к построению менее сложных моделей. Алгоритм относительно устойчив к разным значениям alpha. Это означает, что значение alpha не оказывает значительного влияния на хорошую работу модели. Вместе с тем тонкая настройка этого параметра обычно немного увеличивает правильность.

GaussianNB в основном используется для данных с очень высокой размерностью, тогда как остальные наивные байесовские модели широко используются для разреженных дискретных данных, например, для текста. MultinomialNB обычно работает лучше, чем BernoulliNB, особенно на наборах данных с относительно большим количеством признаков, имеющих ненулевые частоты (т.е. на больших документах).
Наивные байесовские модели разделяют многие преимущества и недостатки линейных моделей. Они очень быстро обучаются и прогнозируют, а процесс обучения легко интерпретировать. Модели очень хорошо работают с высокоразмерными разреженными данными и относительно устойчивы к изменениям параметров. Наивные байесовские модели являются замечательными базовыми моделями и часто используются на очень больших наборах данных, где обучение даже линейной модели может занять слишком много времени.

### Деревья решений
  
Деревья решений являются моделями, широко используемыми для решения задач классификации и регрессии. 


### Оценка неопределенности для классификаторов

 степень уверенности модели в правильности прогноза    
Функиии оценки неопределенности прогнозов:
- decision_function
- predict_proba.


# краткий обзориспользования моделей:
Ближайшие соседи  
Подходит для небольших наборов данных, хорош в качестве базовой модели, прост в объяснении.
  
Линейные модели  
Считается первым алгоритмом, который нужно попробовать, хорош для очень больших наборов данных, подходит для данных с очень высокой размерностью.
  
Наивный байесовский классификатор  
Подходит только для классификации. Работает даже быстрее, чем линейные модели, хорош для очень больших наборов данных и высокоразмерных данных. Часто менее точен, чем линейные модели.
  
Деревья решений  
Очень быстрый метод, не нужно масштабировать данные, результаты можно визуализировать и легко объяснить.
  
Случайные леса  
Почти всегда работают лучше, чем одно дерево решений, очень устойчивый и мощный метод. Не нужно масштабировать данные. Плохо работает с данными очень высокой размерности и разреженными данными.
  
Градиентный бустинг деревьев решений  
Как правило, немного более точен, чем случайный лес. В отличие от случайного леса медленнее обучается, но быстрее предсказывает и требует меньше памяти. По сравнению со случайным лесом требует настройки большего числа параметров.
  
Метод опорных векторов  
Мощный метод для работы с наборами данных среднего размера и признаками, измеренными в едином масштабе. Требует масштабирования данных, чувствителен к изменению параметров.
  
Нейронные сети  
Можно построить очень сложные модели, особенно для больших наборов данных. Чувствительны к масштабированию данных и выбору параметров. Большим моделям требуется много времени для обучения.


# Основные алгоритмы обучения без учителем

Типы алгоритмов:
- Преобразование данных 
- кластеризация