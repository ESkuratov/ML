Основные алгоритмы обучения с учителем:
- классификация 
- регрессия
  
Цель классификации состоит в том, чтобы спрогнозировать метку класса (class label), которая представляет собой выбор из заранее определенного списка возможных вариантов.

Цель регрессии состоит в том, чтобы спрогнозировать непрерывное число или число с плавающей точкой (floating-point number)

Виды классификации:
- бинарная
  - положительный класс (определяется объектом исслудования)
  - отрицательный класс 
- мультиклассовая

Структура данных для классификации
- классы
  - обобщенные группы в данных(сорты ростений, виды животных)
- метки
  - названия групп 
- пример(sample)/точкой данных(data point) - строка/объект в данных 
- признак(features) - столбец/свойство описывает/характеризует пример
  - высокоразмерность - большое количество признаков
  - низкоразмерность - небольшое количестов признаов
  - конструирование признаков(feature engineering)- включение производных признаков

Характеристика модели:
- переобучение(overfitting)
  - точно прогназирует на тестовых данных, делает плохой прогноз на новых данных
- недообучение(underfitting)
  - плохой прогноз на тестовых данных
- максимальная способность обобщать (generalize)
  - делает точный прогноз на новых данных

![[Vvedenie_v_mashinnoe_obuchenie_s_pomoschyu_Python_.png]]

> сложность модели тесно связана с изменчивостью входных данных, содержащихся в вашем обучающем наборе: чем больше разнообразие точек данных в вашем наборе, тем более сложную модель можно использовать, не беспокоясь о переобучении. Обычно больший объем данных дает большее разнообразие, таким образом, большие наборы данных позволяют строить более сложные модели. Однако простое дублирование одних и тех же точек данных или сбор очень похожих данных здесь не поможет.

## Метод К ближайших соседей

простейший вариант алгоритма к ближайших соседей рассматривает лишь одного ближайшего соседа-точку обучающего набора, ближе всего расположенную к точке, для которой мы хотим получить прогноз. 
В простейшем варианте алгоритм k ближайших соседей рассматривает лишь одного ближайшего соседа – точку обучающего набора, ближе всего расположенную к точке, для которой мы хотим получить прогноз. Прогнозом является ответ, уже известный для данной точки обучающего набора.

можем рассмотреть произвольное количество (k) соседей. Отсюда и происходит название алгоритма k ближайших соседей. Когда мы рассматриваем более одного соседа, для присвоения метки используется голосование (voting).

Это означает, что для каждой точки тестового набора мы подсчитываем количество соседей, относящихся к классу 0, и количество соседей, относящихся к классу 1. Затем мы присваиваем точке тестового набора наиболее часто встречающийся класс: другими словами, мы выбираем класс, набравший большинство среди k ближайших соседей.

В случае мультиклассовой классификации мы подсчитываем
количество соседей, принадлежащих к каждому классу, и снова прогнозируем наиболее часто встречающийся класс.

При использовании алгоритма ближайших соседей важно выполнить предварительную обработку данных (смотрите главу 3). Данный метод не так хорошо работает, когда речь идет о наборах данных с большим количеством признаков (сотни и более), и особенно плохо работает в ситуации, когда подавляющее число признаков в большей части наблюдений имеют нулевые значения (так называемые разреженные наборы данных или sparse datasets).

### Регрессия К ближайших соседей


## Линейные модели

Линейные модели дают прогноз, используя линейную функцию (linear function) входных признаков

Когда используется несколько признаков,
регрессионное уравнение содержит параметры наклона для каждого признака. Как вариант, прогнозируемый ответ можно представить в виде взвешенной суммы входных признаков, где веса (которые могут быть отрицательными) задаются элементами w.


Линейные модели для регрессии можно охарактеризовать как регрессионные модели, в которых прогнозом является прямая линия для одного признака, плоскость, когда используем два признака, или гиперплоскость для большего количества измерений (то есть, когда используем много признаков).


Для наборов данных с большим количеством признаков линейные модели могут быть очень полезны. В частности, если у вас количество признаков превышает количество точек данных для обучения, любую целевую переменную у можно прекрасно смоделировать (на обучающей выборке) в виде линейной функциию  

Существует различные виды линейных моделей для регрессии. Различие между этими моделями заключается в способе оценивания параметров модели w и b по обучающим данным и контроле сложности модели.  

### Линейная регрессия( обычный метод наименьших квадратов)
**Оrdinary least squares, OLS**  

Линейная регрессия находит параметры w и b, которые минимизируют среднеквадратическую ошибку (mean squared error) между спрогнозированными и фактическими ответами у в обучающем наборе.  

>  Среднеквадратичная ошибка равна сумме квадратов разностей между спрогнозированными и фактическими значениями.  


Линейная регрессия проста, что является преимуществом, но в то же время у нее нет инструментов, позволяющих контролировать сложность модели.    

Однако для высокоразмерных наборов данных (наборов данных с большим количеством признаков) линейные модели становятся более сложными и существует более высокая вероятность переобучения.  

Это несоответствие между качеством модели на обучающем наборе и качеством модели на тестовом наборе является явным признаком переобучения  

### Гребневая регрессия

Гребневая регрессия также является линейной моделью регрессии, поэтому ее формула аналогична той, что используется в обычном методе наименьших квадратов.  

В гребневой регрессии коэффициенты (w) выбираются не только с точки зрения того, насколько хорошо они позволяют предсказывать на обучающих данных, они еще подгоняются в соответствии с дополнительным ограничением.  


Нам нужно, чтобы величина коэффициентов была как можно меньше. Другими словами, все элементы w должны быть близки к нулю. Это означает, что каждый признак должен иметь как можно меньшее влияние на результат (то есть каждый признак должен иметь небольшой регрессионный коэффициент) и в то же время он должен по-прежнему обладать хорошей прогнозной силой.  

Это ограничение является примером регуляризации (regularization).   

Регуляризация означает явное ограничение модели для предотвращения переобучения. Регуляризация, использующаяся в гребневой регрессии, известна как L2 регуляризация  


